# std lib imports
from __future__ import annotations
from typing import Literal, Optional
from dataclasses import dataclass

# 3-party imports 
import numpy as np

# base imports
from xai_bench.models.base_model import BaseModel
from xai_bench.stat_collector import StatCollector

# Options
TaskType = Literal["classification", "regression"]
OutputSpace = Literal["model", "raw"]
#
@dataclass
class Features:
    feature_names_model: list[str]

class Explanation:
    """
        Standardized explanation object to encapsulate feature attributions.

        This class serves as a container for feature attributions generated by
        explainers. It holds the attribution values along with metadata such as
        feature names.

        Args:
            values (np.ndarray):
                A 2D array of shape (n_samples, n_features) containing the feature
                attributions for each sample.

            feature_names (list[str]):
                A list of feature names corresponding to the columns in `values`.
    """

    def __init__(self, values: np.ndarray, feature_names: list[str]):
        self.values = values
        self.feature_names = feature_names

class BaseExplainer:
    """
        Abstract base class for all explanation methods used in the benchmark.

        This class defines the minimal interface that every explainer (e.g. LIME, SHAP)
        must implement in order to be compatible with the evaluation pipeline,
        attack implementations, and metrics.

        The goal of this base class is to enforce a uniform contract:
        - how explainers are initialized (`fit`)
        - how explanations for individual samples are generated (`explain`)
        - what information an explanation must contain (via the `Explanation` object)

        Concrete explainers must subclass this class and implement the `explain` method.
    
    """
    def __init__(self,stats:tuple):
        self.stats = StatCollector(stats[0],comment=stats[1])
        self.num_samples = 0

    def fit(
        self, 
        reference_data: np.ndarray, 
        model: BaseModel, 
        features: Features
    ) -> None:
        """  
            Initialize the explainer with all global information required to produce
            local explanations.

            This method is called once before explanations are generated. It provides
            the explainer with:
            - reference_data (e.g. for sampling or expectations),
            - a model defining the model to be explained,
            - a feature specification describing the input feature space.

            The exact use of the reference_data depends on the explainer:
            - For SHAP, it typically defines the background distribution.
            - For LIME, it defines the data distribution used for perturbations.
            - Some explainers may ignore it entirely.

            Args:
                reference_data (np.ndarray):
                    reference_data expressed in the same feature space as the model input.

                model:
                    A model specification or wrapper object that exposes a scalar
                    prediction function to be explained (e.g. probability for a
                    target class or regression output).

                features:
                    A feature space specification describing the model input features,
                    including feature names and optional mappings between model-space
                    and raw feature representations.
        """
        raise NotImplementedError


    def explain(
        self, 
        X: np.ndarray,
        num_samples: Optional[int] = None
    ) -> np.ndarray:
        """
            Generate a local explanation for a single input sample.

            This method computes feature attributions explaining the model's prediction
            for the given input `x`. The explanation must be returned in a standardized
            `Explanation` object to ensure compatibility with attacks and metrics.

            Args:
                X (np.ndarray):
                    Input sample with shape (n, d), expressed in the
                    model input feature space.

            Returns:
                np.ndarray:
                    Feature attributions of shape (n, d) corresponding to the input sample,
                    where each value indicates the importance of the respective feature.
                    Encoded features' attributions are summarized according to the dataset's
                    feature mapping.

            Raises:
                NotImplementedError:
                    If the method is not implemented by a subclass.
        """
        raise NotImplementedError

